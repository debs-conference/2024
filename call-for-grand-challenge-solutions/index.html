---
# this is an empty front matter
---
<!DOCTYPE html>
<html lang="en">

<head>
    <title>CALL FOR GRAND CHALLENGE SOLUTIONS</title>
    {% include head-tags.html %}
</head>

<body>

    {% include header.html %}
    {% include slider-regular-page.html title="CALL FOR GRAND CHALLENGE SOLUTIONS" %}

    <section id="teaser-call-for-grand-challenge-solutions">
        <div class="container">
            <div class="row">
                <div class="col-md-8 col-sm-12">
                    <div class="block">
                        <div>
                            <!-- <h1>Teaser 2024: Call for Grand Challenge Solutions</h1>

                            <p>
                                The DEBS Grand Challenge is a series of competitions that started in 2010, in which both
                                participants from academia and industry compete with the goal of building faster and more
                                scalable distributed and event-based systems that solve a practical problem. Every year, the
                                DEBS Grand Challenge participants have a chance to explore a new dataset and a new problem
                                and can compare their results based on the common evaluation criteria. The winners of the
                                challenge are announced during the conference where they are competing for a performance
                                and an audience award. Apart from correctness and performance, submitted solutions are also
                                assessed along a set of non-functional requirements.
                            </p>

                            <p>
                                The 2024 DEBS Grand Challenge focuses on real-time complex event processing of real-world
                                telemetry data provided by Backblaze (<a href="https://www.backblaze.com/">https://www.backblaze.com/</a>).The dataset used for the
                                Grand Challenge is based on fine-granular telemetry data about over 200k hard drives in data
                                centers operated by Backblaze. The goal of the challenge is to identify models that exhibit
                                comparable behavior with respect to a set of attributes best suited to predict failures.
                                Further details on the dataset provided, the queries, non-functional requirements and the
                                overall submission process can be found here: <a href="https://2024.debs.org/call-for-grand-challenge-solutions/">https://2024.debs.org/call-for-grand-challenge-solutions/</a>.
                            </p> -->

                            <h1>DEBS 2024: Call for Grand Challenge Solutions</h1>
                            
                            <p>
                                The DEBS Grand Challenge is a series of competitions that started in 2010, in which both participants from academia and industry compete with the goal of building faster and more scalable distributed and event-based systems that solve a practical problem. Every year, the DEBS Grand Challenge participants have a chance to explore a new data set and a new problem and can compare their results based on the common evaluation criteria. The winners of the challenge are announced during the conference where they are competing for a performance and an audience award. Apart from correctness and performance, submitted solutions are also assessed along a set of non-functional requirements.
                            </p>

                            <h1>
                                Topic: Telemetry data for hard drive failure prediction & predictive maintenance
                            </h1>

                            <p>
                                The 2024 DEBS Grand Challenge focuses on real-time processing of real-world telemetry data provided by Backblaze (<a href="https://www.backblaze.com/">https://www.backblaze.com/</a>).The data set used for the Grand Challenge contains fine-granular telemetry data about over 200k hard drives in data centers operated by Backblaze. The goal of the challenge is to continuously compute clusters of similar drives as new data is received.
                                Further details on the data set provided, the queries, requirements and the overall submission process can be found here: <a href="https://2024.debs.org/call-for-grand-challenge-solutions/">https://2024.debs.org/call-for-grand-challenge-solutions/</a>
                            </p>

                        </div>
                        <div>
                            <h1>Participation</h1>

                            <p>
                                Participation in the DEBS 2024 Grand Challenge consists of three steps: (1) registration, (2) iterative solution submission, and (3) paper submission.
                            </p>

                            <!-- <p>
                                The first step is to pre-register your submission by sending an abstract at the central DEBS 2024
                                conference management system in the "Grand Challenge Track" and send an email to one of the
                                Grand Challenge Co-Chairs (see <a href="https://2024.debs.org/organizing-committee/">https://2024.debs.org/organizing-committee/</a>).
                            </p> -->
                            <!-- <p>      
                                Solutions to the challenge, once developed, must be submitted to the evaluation plaVorm
                                (details will be available shortly) in order to get it benchmarked in the challenge. The evaluation
                                platform provides detailed feedback on performance and allows to update the solution in an
                                iterative process. A solution can be continuously improved until the challenge closing date.
                            </p>
                            <p>
                                The last step is to upload a short paper (maximum 6 pages) describing the final solution via the
                                central DEBS 2024 conference management system. All papers will be reviewed by the DEBS
                                Grand Challenge Committee to assess the merit and originality of submitted solutions. Accepted
                                papers will be included in the DEBS 2024 proceedings and will be presented during the poster
                                session at the conference.
                            </p> -->

                            <p>
                                The first step is to pre-register your submission by registering your abstract at  Easychair <a href="https://easychair.org/my/conference?conf=debs24">https://easychair.org/my/conference?conf=debs24</a>  in the "Grand Challenge Track'' and send an email to one of the Grand Challenge Co-Chairs at <a href="mailto:debs24gc@gmail.com">debs24gc@gmail.com</a>  (see <a href="https://2024.debs.org/organizing-committee/">https://2024.debs.org/organizing-committee/</a>  or the eval platform landing page). Solutions to the challenge, once developed, must be submitted to the evaluation platform (<a href=" https://challenge2024.debs.org/"> https://challenge2024.debs.org/</a>) to get benchmarked in the challenge. The evaluation platform provides detailed feedback on performance and allows the solution to be updated in an iterative process. A solution can be continuously improved until the challenge closing date. Evaluation results of the last submitted solution will be used for the final performance ranking. The last step is to upload a short paper (minimum 2 pages, maximum 6 pages) describing the final solution via the central conference management tool Easychair. The DEBS Grand Challenge Committee will review all papers to assess the merit and originality of submitted solutions. All solutions of sufficient quality will be presented during the poster session at the DEBS 2024 conference.
                            </p>

                            <h1>Queries</h1>
                            <p>
                                This yearâ€™s DEBS Grand Challenge requires you to implement a count of the recent number of failures detected for each vault (group of storage servers) (Query Q1) and use this number to continuously compute a cluster of the drives (Query Q2).
                            </p>

                            <h2>Input</h2>
                            <p>
                                Input data consists of batches and each batch contains:
                                <ul class="nested-list">
                                    <li>SMART readings for a list of drives</li>
                                    <li>vault_ids: a list of vault identifiers of interest for this batch (used in Q1, see below)</li>
                                    <li>cluster_ids: a list of cluster identifiers of interest for this batch (used in Q2, see below)</li>
                                    <li>day_end: a flag that marks the end of one day of readings</li>
                                </ul>
                            </p>

                            <h2>Query 1</h2>
                            <p>
                                <ul class="nested-list">
                                <li>
                                    For every vault v, count the number of failures NF_v in a sliding window W
                                    <ul class="nested-list2">
                                        <li>Size = 30 days</li>
                                        <li>Slide = 1 day</li>
                                    </ul>
                                </li>
                                <li>For a given day i, NF_v^i is the count of failures in the window that starts at i-31 (included) and ends at i-1 (included)</li>
                                <li>The first window closes at day 0: you can assume 0 failures for every day i &lt= 0</li>
                                <li>Each batch of input data will contain the identifiers of 5 vaults (field vault_ids): you are to return the current value of NF_v for those vaults as the query 1 response for the corresponding batch (Note: as NF_v^i uses the readings up to day i-1, the result can be sent for each batch without waiting for the end of the current day)</li>
                                </ul>
                            </p>

                            <h2>Query 2</h2>

                            <p>
                                <ul class="nested-list">
                                    <li>
                                        For each reading at day i regarding a drive d belonging to vault v, add NF_v^i to the reading
                                    </li>
                                    <li>
                                        Normalize data
                                        <ul class="nested-list2">
                                            <li>
                                                Rescale the smart values within the provided ranges
                                            </li>

                                            <li>
                                                The ranges can be downloaded from the <a href="https://challenge2024.debs.org/">challenger platform</a>
                                            </li>
                                        </ul>
                                    </li>
                                    <li>Compute dynamic K-means clustering
                                        <ul class="nested-list2">
                                            <li>
                                                Assign incoming readings to the nearest centroid
                                            </li>
                                            <li>
                                                At the end of one day, which is marked by a â€˜day_endâ€™ flag in the batch, update the centroids positions with the average coordinates of all the readings currently associated to that centroid
                                            </li>
                                            <li>
                                                The initial coordinates of centroids are provided, and can be downloaded from the <a href="https://challenge2024.debs.org/">challenger platform</a>
                                            </li>
                                        </ul>
                                    </li>

                                    <li>
                                        Each batch of input data will contain a list of cluster identifiers (clusters are identified by a sequence number): you are to return the number of drives associated to each of those clusters
                                    </li>
                                </ul>
                            </p>
                            <h2>Dataset Provided</h2>
                            <p>
                                The data provided for this DEBS Grand Challenge is based on S.M.A.R.T (<a href="https://en.wikipedia.org/wiki/Self-Monitoring,_Analysis_and_Reporting_Technology">https://en.wikipedia.org/wiki/Self-Monitoring,_Analysis_and_Reporting_Technology</a>) data and additional attributes captured by Backblaze on a daily basis for six months (starting Q2 2023).
                            </p>
                            <p>
                                The data set contains events covering over 200k hard disks. Each data point resembles the S.M.A.R.T. status of a dedicated disk on a specific day.
                            </p>
                            <p>
                                The full data set will be used for benchmarking the submitted solutions. The smaller test data set will be provided upfront via our eval platform for testing purposes. The test data set contains event notifications representing six months of data about drives only from a single manufacturer. Consequently, participants must not pay attention to filtering out event notifications that do not contain attributes relevant to the Grand Challenge.
                            </p>
                            <p>
                                The events are provided through a GRPC based API (see the protobuf definition below). The input data is provided in numbered Batches of DriveState. Each DriveState is a reading identified by a timestamp and the serial number of the drive. The reading contains the model of the drive, the id of the vault where it is located and a boolean marking whether it has failed. The DriveState contains a list of readings for the smart attributes of the drive at that specific timestamp. It is provided as a list of int64 raw readings, the values map to the smart attributes in the following order: s1,s2,s3,s4,s5,s7,s8,s9,s10,s12,s173,s174,s183,s187,s188,s189,s190,s191,s192,s193,s194,s195,s196,s197,s198,s199,s200,s220,s222,s223,s226,s240,s241,s242
                            </p>
                            <pre>
                                <code>
    syntax = "proto3";

    import "google/protobuf/empty.proto";
    import "google/protobuf/timestamp.proto";
    
    option java_multiple_files = true;
    option java_package = "org.debs.gc2023.bandency";
    
    package Challenger;
    
    message DriveState {
        google.protobuf.Timestamp date = 1;
        string serial_number = 2;
        string model = 3;
        bool failure = 4;
        int32 vault_id = 5;
        // SMART raw readings in the following order:
        // s1,s2,s3,s4,s5,s7,s8,s9,s10,
        // s12,s173,s174,s183,s187,s188,
        // s189,s190,s191,s192,s193,s194,
        // s195,s196,s197,s198,s199,s200,
        // s220,s222,s223,s226,s240,s241,
        // s242      
        repeated int64 readings = 6;
    }
    
    message Batch {
        int64 seq_id = 1;
        bool last = 2;
        bool day_end = 3;
        repeated int32 vault_ids = 4;
        repeated int32 cluster_ids = 5;
        repeated DriveState states = 6;
    }
    
    message Benchmark {
        int64 id = 1;
    }
    
    message VaultFailures {
        int32 vault_id = 1;
        int32 failures = 2;
    }
    
    message ResultQ1 {
        int64 benchmark_id = 1;
        int64 batch_seq_id = 2;
    
        repeated VaultFailures entries = 3;
    }
    
    message ClusterInfo {
        int32 cluster_id = 1;
        int32 size = 2;
    }
    
    message ResultQ2 {
        int64 benchmark_id = 1;
        int64 batch_seq_id = 2;
    
        repeated ClusterInfo entries = 3;
    }
    
    enum Query {
        Q1 = 0;
        Q2 = 1;
    }
    
    message BenchmarkConfiguration {
        string token = 1; // Token from the webapp for authentication
        string benchmark_name = 2; // chosen by the team, listed in the results
        string benchmark_type = 3; // benchmark type, e.g., test
        repeated Query queries = 4; // Specify which queries to run
    }
    
    service Challenger {
    
        //Create a new Benchmark based on the configuration
        rpc createNewBenchmark(BenchmarkConfiguration) returns (Benchmark);
    
        //This marks the starting point of the throughput measurements
        rpc startBenchmark(Benchmark) returns (google.protobuf.Empty);
    
        //get the next Batch
        rpc nextBatch(Benchmark) returns (Batch);
    
        //post the result
        rpc resultQ1(ResultQ1) returns (google.protobuf.Empty);
        rpc resultQ2(ResultQ2) returns (google.protobuf.Empty);
        
        //This marks the end of the throughput measurements
        rpc endBenchmark(Benchmark) returns (google.protobuf.Empty);
    }
                                    
                                </code>
                            </pre>
                            <q3>HTTP Accessibility API Example</q3>
                            <pre>
                                <code>
    POST /create HTTP/1.1
    Content-Length: 96
    Content-Type: application/json
    Host: 127.0.0.1:3000
    User-Agent: HTTPie
    
    
    {
        "token": "pepega",
        "benchmark_name": "asd",
        "benchmark_type": "test",
        "queries": [0]
    }
    
    
    POST /start HTTP/1.1
    Content-Length: 18
    Content-Type: application/json
    Host: 127.0.0.1:3000
    User-Agent: HTTPie
    
    
    {
        "id": 734652
    }
    
    
    POST /end HTTP/1.1
    Content-Length: 18
    Content-Type: application/json
    Host: 127.0.0.1:3000
    User-Agent: HTTPie
    
    
    {
        "id": 734652
    }
    
    
    POST /next_batch HTTP/1.1
    Content-Length: 18
    Content-Type: application/json
    Host: 127.0.0.1:3000
    User-Agent: HTTPie
    
    
    {
        "id": 734652
    }
    
    
    POST /result_q1 HTTP/1.1
    Content-Length: 136
    Content-Type: application/json
    Host: 127.0.0.1:3000
    User-Agent: HTTPie
    
    
    {
        "benchmark_id": 734652,
        "batch_seq_id": 123,
        "entries": [
        {
            "model": "AB12",
            "intervals": ["a", "b"]
        }
        ]
    }
    
    
    POST /result_q2 HTTP/1.1
    Content-Length: 104
    Content-Type: application/json
    Host: 127.0.0.1:3000
    User-Agent: HTTPie
    
    
    {
        "benchmark_id": 734652,
        "batch_seq_id": 123,
        "centroids_out": [1, 2],
        "centroids_in": [3, 4]
    }
                                                                  
    
                                </code>
                              </pre>
                        </div>

                        <div>
                            <h1>Important Dates</h1>
                            <!-- <li>
                                Release of the challenge, initial data set: <b>November 6th, 2023</b>
                            </li> -->
                            <li>
                                Test platform opens: <b>January, 2023</b>
                            </li>
                            <li>
                                Evaluation platform opens: <b>March, 2024</b>
                            </li>
                            <li>
                                Deadline for uploading final solution to the evaluation platform: <b>May 12th, 2024</b>
                            </li>
                            <li>
                                Evaluation of the solution: <b>May 19th, 2024</b>
                            </li>
                            <li>
                                Deadline for short paper submission: <b>June 2nd, 2024</b>
                            </li>
                            <li>
                                Notification of acceptance: <b>June 9th, 2024</b>
                            </li>
                            <li>
                                Camera ready submission: <b>June 16th, 2024</b>
                            </li>
                            <li>
                                Conference <b>June 25th â€“ 28th June, 2024</b>
                            </li>
                        </div>

                        <div>
                            {% if site.data.chairs.grand_challenge_chairs %}
                            <h1>Grand Challenge Co-Chairs</h1>
                            {% for chair in site.data.chairs.grand_challenge_chairs %}
                            <li>{{ chair.firstname }} {{ chair.lastname }}, {{ chair.institution }}</li>
                            {% endfor %}
                            {% endif %}
                        </div>
                    </div>
                </div>

                {% include side.html %}
            </div>
    </section>

    {% include footer.html %}
</body>
</html>